{
  "//": "Folders of the data to train on and to save the model to",
  "data_dir": "./data",
  "output_dir": "../output",

  "//": "HuggingFace credentials",
  "hf_token": null,
  "hf_repo_name": null,

  "//": "Model name and parameters",
  "model_name": "EleutherAI/gpt-neo-125m",
  "load_in_4bit": false,
  "load_in_8bit": false,
  "model_max_length": 1024,

  "//": "Training arguments",
  "batch_size": 1,
  "gradient_accumulation_steps": 1,
  "learning_rate": 0.0001,
  "num_train_epochs": 3,
  "logging_steps": 1,

  "//": "Parameter efficient fine tuning (PEFT) arguments",
  "use_peft": true,
  "peft_lora_r": 16,
  "peft_lora_alpha": 16
}
